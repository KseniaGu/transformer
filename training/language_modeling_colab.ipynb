{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LanguageModel.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.insert(0,'/content/drive/MyDrive/NLP/Transformer')"
      ],
      "metadata": {
        "id": "UFbVrZazSPaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from model.transformer import Transformer"
      ],
      "metadata": {
        "id": "kawY07iiSpuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup:"
      ],
      "metadata": {
        "id": "ppGd1ZlxLZKT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "!pip install transformers\n",
        "!pip install pytorch-lightning\n",
        "!pip3 install Cython\n",
        "'''"
      ],
      "metadata": {
        "id": "Vw71igZ9LlVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torchtext.datasets import WikiText2\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torch.utils.data import Dataset, RandomSampler, DataLoader\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning import seed_everything\n",
        "from transformers import AutoModelForSeq2SeqLM, BartConfig, AutoConfig\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "SvOzvKgqLb01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_cfg = {\n",
        "    'input_vocab_size':28782,\n",
        "    'output_vocab_size':28782,\n",
        "    'device': torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
        "    'dropout': 0.2,\n",
        "    'emb_size': 512,\n",
        "    'hidden_size': 64,\n",
        "    'nrof_heads': 2,\n",
        "    'f_hidden_size': 64,\n",
        "    'nrof_layers': 2\n",
        "}"
      ],
      "metadata": {
        "id": "jeWzjmhYKkP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data"
      ],
      "metadata": {
        "id": "G1h9uv18KGji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WikiTextDataset(Dataset):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.phase = 'train'\n",
        "        self.dataset = {}\n",
        "\n",
        "    def preprocess(self, raw_text_iter):\n",
        "        data = [torch.tensor(self.vocab(self.tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n",
        "        return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
        "    \n",
        "    def batchify(self, data, seq_len=513, device='cpu'):\n",
        "        nrof_seqs = data.size(0) // seq_len\n",
        "        data = data[:nrof_seqs * seq_len]\n",
        "        data = data.view(nrof_seqs, seq_len)\n",
        "        return data.to(device)\n",
        "\n",
        "    def prepare(self):\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        \n",
        "        self.tokenizer = get_tokenizer('basic_english')\n",
        "        self.vocab = build_vocab_from_iterator(map(\n",
        "            self.tokenizer, WikiText2(split='train')), specials=['<unk>'])\n",
        "        self.vocab.set_default_index(self.vocab['<unk>'])\n",
        "\n",
        "        train_iter, val_iter, test_iter = WikiText2()\n",
        "        self.dataset['train'] = self.batchify(self.preprocess(train_iter), \n",
        "                                              device=device)\n",
        "        self.dataset['val'] = self.batchify(self.preprocess(val_iter), \n",
        "                                            device=device)\n",
        "        self.dataset['test'] = self.batchify(self.preprocess(test_iter), \n",
        "                                             device=device)\n",
        "\n",
        "    def set_phase(self, phase='train'):\n",
        "        self.phase = phase\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset[self.phase]) - 1\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # returns both: inputs for encoder and decoder\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.item()\n",
        "\n",
        "        item = self.dataset[self.phase][idx][:-1]\n",
        "        target = self.dataset[self.phase][idx][1:]\n",
        "        return item, item, target\n"
      ],
      "metadata": {
        "id": "vwW9ndIGFU5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_mask(batched_sequence, decoding=False):\n",
        "    '''\n",
        "    batched_seq of shape (b_s, max_seq_len, emb_size)\n",
        "    '''\n",
        "    b_s, max_seq_len = batched_sequence.shape\n",
        "    mask_pad = batched_sequence.unsqueeze(1).repeat_interleave(max_seq_len, dim=1) != 0\n",
        "    mask_pad = ~ (mask_pad * mask_pad.permute(0,2,1))\n",
        "    if decoding:\n",
        "        mask = torch.full((b_s, max_seq_len, max_seq_len), True)\n",
        "        mask = torch.triu(mask, diagonal=1) \n",
        "        mask[mask_pad] = True\n",
        "        mask[mask.prod(dim=1)==1] = False\n",
        "        return mask \n",
        "    mask_pad[mask_pad.prod(dim=1)==1] = False\n",
        "    return mask_pad\n",
        "\n",
        "def my_collate(batch):\n",
        "    input_encoder, input_decoder, target = zip(*batch)\n",
        "    input_encoder = torch.vstack(input_encoder)\n",
        "    input_decoder = torch.vstack(input_decoder)\n",
        "    target = torch.vstack(target)\n",
        "    encoder_mask = get_mask(input_encoder)\n",
        "    decoder_mask = get_mask(input_decoder, decoding=True)\n",
        "    data = {\n",
        "        'input_encoder': input_encoder,\n",
        "        'input_decoder': input_decoder,\n",
        "        'encoder_mask': encoder_mask,\n",
        "        'decoder_mask': decoder_mask\n",
        "    }\n",
        "\n",
        "    return data, target\n",
        "\n",
        "def get_dataloader(dataset, sampler=None, phase='train', batch_size=4):\n",
        "    if phase == 'train' and sampler is None:\n",
        "        sampler = RandomSampler(dataset)\n",
        "\n",
        "    dataloader = DataLoader(dataset,\n",
        "                            sampler=sampler,\n",
        "                            batch_size=batch_size,\n",
        "                            drop_last=phase == 'train',\n",
        "                            collate_fn=my_collate)\n",
        "    return dataloader"
      ],
      "metadata": {
        "id": "Am6MLRmyPWHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train with pytorch lightening:"
      ],
      "metadata": {
        "id": "zHRoUu71MPr3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerLM(pl.LightningModule):\n",
        "    def __init__(self, model_config):\n",
        "        super(TransformerLM, self).__init__()\n",
        "        self.model_config = model_config\n",
        "        self.model = Transformer(model_config)\n",
        "        self.model = self.model.to(model_cfg['device'])\n",
        "        self.dataset = WikiTextDataset()\n",
        "\n",
        "    def prepare_data(self):\n",
        "        self.dataset.prepare()\n",
        "\n",
        "    def forward(self, **data):\n",
        "        return self.model(**data)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        data, target = batch\n",
        "        output = self(**data)\n",
        "        loss = F.cross_entropy(output.view(\n",
        "            -1, self.model_config['output_vocab_size']), target.view(-1))\n",
        "        tensorboard_logs = {'train_loss': loss}\n",
        "        return {'loss': loss, 'log': tensorboard_logs}\n",
        "\n",
        "    def validation_step(self, batch, batch_nb):\n",
        "        data, target = batch\n",
        "        output = self(**data)\n",
        "        loss = F.cross_entropy(output.view(\n",
        "            -1, self.model_config['output_vocab_size']), target.view(-1))\n",
        "        _, preds = torch.max(output, -1)\n",
        "        val_acc = accuracy_score(preds.cpu().view(-1), target.cpu().view(-1))\n",
        "        val_acc = torch.tensor(val_acc)\n",
        "\n",
        "        return {'val_loss': loss, 'val_acc': val_acc}\n",
        "\n",
        "    def validation_epoch_end(self, outputs):\n",
        "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
        "        avg_val_acc = torch.stack([x['val_acc'] for x in outputs]).mean()\n",
        "\n",
        "        tensorboard_logs = {'val_loss': avg_loss, 'avg_val_acc': avg_val_acc}\n",
        "        return {'val_loss': avg_loss, 'progress_bar': tensorboard_logs}\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(\n",
        "            [p for p in self.parameters() if p.requires_grad], lr=2e-05, eps=1e-08)\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        return get_dataloader(self.dataset)\n",
        "    \n",
        "    def val_dataloader(self):\n",
        "        self.dataset.set_phase('val')\n",
        "        return get_dataloader(self.dataset, phase='val')\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        self.dataset.set_phase('test')\n",
        "        return get_dataloader(self.dataset, phase='test')"
      ],
      "metadata": {
        "id": "FJ2m0AulMWqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed_everything(11)\n",
        "\n",
        "logger = pl.loggers.TensorBoardLogger(\"tb_logs\", name=\"transformer_ml\")\n",
        "\n",
        "transformer_lm = TransformerLM(model_cfg)\n",
        "transformer_lm.prepare_data()\n",
        "\n",
        "trainer = pl.Trainer(max_epochs=10, gpus=1, logger=logger)    \n",
        "trainer.fit(transformer_lm) "
      ],
      "metadata": {
        "id": "7bQKYGGFskw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from shutil import rmtree\n",
        "import os\n",
        "\n",
        "# to clean logs dir while debugging\n",
        "for version in os.listdir('/content/lightning_logs'):\n",
        "    rmtree(os.path.join('/content/lightning_logs', version))"
      ],
      "metadata": {
        "id": "7aGxDlVoL0T4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir tb_logs/transformer_ml/"
      ],
      "metadata": {
        "id": "hJVcIVqvJ3Wd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
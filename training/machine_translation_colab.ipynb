{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TransformerMT.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "QPD8RNxOl0Nf",
        "C5-zYm3MbTl2"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvr-GlW7Zkq-"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.insert(0,'/content/drive/MyDrive/NLP/Transformer')"
      ],
      "metadata": {
        "id": "taKKgTfGXDbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from model.transformer import Transformer"
      ],
      "metadata": {
        "id": "45ZjGaHaXFob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tdnwd2O88A9t"
      },
      "source": [
        "import torch\n",
        "import io \n",
        "from torch import nn\n",
        "from math import sqrt\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.utils.data.sampler import Sampler\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "from  torch.optim.lr_scheduler import LambdaLR, StepLR\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import os \n",
        "from time import time \n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "from time import time \n",
        "import copy\n",
        "\n",
        "torch.random.manual_seed(0)\n",
        "cfg_train_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWGyqU68Hpl6"
      },
      "source": [
        "data_path = '/content/drive/MyDrive/NLP/Transformer' \n",
        "anki_data_path =  '/content/drive/MyDrive/NLP/Transformer/data/anki_rus.txt' \n",
        "\n",
        "cfg_exp_name = 'anki' # from ['digits', 'anki']\n",
        "cfg_train_init_lr = 1. if cfg_exp_name == 'digits' else 2.  # for CE was 0.05\n",
        "cfg_train_ckpt_path = os.path.join(data_path, cfg_exp_name, 'checkpoints', 'checkpoint')\n",
        "cfg_train_logs_dir = os.path.join(data_path, cfg_exp_name, 'logs')\n",
        "cfg_train_nrof_epochs = 3 if cfg_exp_name == 'digits' else 15\n",
        "cfg_train_log_interval = 100 if cfg_exp_name == 'digits' else 100\n",
        "cfg_train_batch_size = 128\n",
        "cfg_train_train_size = 50000 if cfg_exp_name == 'digits' else None \n",
        "cfg_train_val_size = 1000 if cfg_exp_name == 'digits' else None \n",
        "cfg_train_load = True    \n",
        "cfg_train_warmup_steps = 500 if cfg_exp_name == 'digits' else 4000\n",
        "cfg_train_dropout_prob = 0.2\n",
        "seed_val = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NGLnzR_eOhE"
      },
      "source": [
        "## Utils:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jXf1yULIO2Q"
      },
      "source": [
        "def get_mask(batched_sequence, decoding=False):\n",
        "    '''\n",
        "    batched_seq of shape (b_s, max_seq_len, emb_size)\n",
        "    '''\n",
        "    b_s, max_seq_len = batched_sequence.shape\n",
        "    mask_pad = batched_sequence.unsqueeze(1).repeat_interleave(max_seq_len, dim=1) != 0\n",
        "    mask_pad = ~ (mask_pad * mask_pad.permute(0,2,1))\n",
        "    if decoding:\n",
        "        mask = torch.full((b_s, max_seq_len, max_seq_len), True)\n",
        "        mask = torch.triu(mask, diagonal=1) \n",
        "        mask[mask_pad] = True\n",
        "        mask[mask.prod(dim=1)==1] = False #???\n",
        "        return mask \n",
        "    mask_pad[mask_pad.prod(dim=1)==1] = False\n",
        "    return mask_pad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1g_PstC8H0j"
      },
      "source": [
        "## Data:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPD8RNxOl0Nf"
      },
      "source": [
        "### Digits:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mv96PROZre7n"
      },
      "source": [
        "import random\n",
        "\n",
        "def get_digit_data():\n",
        "    train_digit_sequences = [] #torch.randint(1, 10, (1000, 25)).tolist()\n",
        "    val_digit_sequences = []\n",
        "\n",
        "    for _ in range(cfg_train_train_size):\n",
        "        seq_len = random.randint(5, 25)\n",
        "        train_digit_sequences.append(torch.tensor([1] + np.random.randint(3, 10, seq_len).tolist() + [2]))\n",
        "\n",
        "    for _ in range(cfg_train_val_size):\n",
        "        seq_len = random.randint(5, 25)\n",
        "        val_digit_sequences.append(torch.tensor([1] + np.random.randint(3, 10, seq_len).tolist() + [2]))\n",
        "    \n",
        "    return train_digit_sequences, val_digit_sequences\n",
        "\n",
        "class Decoding:\n",
        "    @staticmethod\n",
        "    def decode(sequence):\n",
        "        return sequence\n",
        "\n",
        "class DigitDataset(Dataset):\n",
        "    def __init__(self, data, vocab_size):\n",
        "        super(DigitDataset, self).__init__()\n",
        "        self.data = data\n",
        "        self.tokenizers = {'input':Decoding(),\n",
        "                           'output':Decoding()}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    @property\n",
        "    def vocab_size(self):        \n",
        "        return {'input': 10,\n",
        "                'output': 10}\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "        \n",
        "        return self.data[idx], self.data[idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5-zYm3MbTl2"
      },
      "source": [
        "### Anki data:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1a7uVyMup859"
      },
      "source": [
        "#### Dataset utils:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hhkb3GOwftcy"
      },
      "source": [
        "!pip install youtokentome"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSZCGkfwrNGc"
      },
      "source": [
        "import youtokentome as yttm\n",
        "from itertools import chain\n",
        "from random import shuffle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jPPiKqhfBtt"
      },
      "source": [
        "def split_anki_dataset():\n",
        "    import sklearn\n",
        "\n",
        "    with open(anki_data_path, 'r') as f:\n",
        "        data = f.readlines()\n",
        "\n",
        "    train, test = sklearn.model_selection.train_test_split(data, test_size=0.2, random_state=11)\n",
        "    val, test = sklearn.model_selection.train_test_split(data, test_size=0.5, random_state=11)\n",
        "    \n",
        "    for phase, data in zip(('train', 'val', 'test'), (train, val, test)):\n",
        "        ru_file_writer = open('ru_' + phase + '_set', 'w')\n",
        "        en_file_writer = open('en_' + phase + '_set', 'w')\n",
        "        for pair in data:\n",
        "            pair_split = pair.split('\\t')\n",
        "            ru_file_writer.write(pair_split[1]+'\\n')\n",
        "            en_file_writer.write(pair_split[0]+'\\n')\n",
        "        ru_file_writer.close()\n",
        "        en_file_writer.close()\n",
        "\n",
        "def chunks(l, n):\n",
        "    max_len = len(l) - len(l)%n\n",
        "    return [l[i:i+n] for i in range(0, max_len, n)]\n",
        "\n",
        "def get_yttm_bpe(data_path, data_type):\n",
        "    tok_path = data_type + '_' +'yttm_tokenizer'\n",
        "    if not os.path.exists(tok_path):\n",
        "        print('BPE training started')\n",
        "        t = time()\n",
        "        tokenizer = yttm.BPE.train(data=data_path, vocab_size=50000, model=tok_path,\n",
        "                                   pad_id=0, unk_id=1, bos_id=2, eos_id=3)\n",
        "        print('BPE trained after {} sec'.format(time() - t))\n",
        "    else:\n",
        "        tokenizer = yttm.BPE(model=tok_path)\n",
        "        print('BPE loaded')\n",
        "    return tokenizer\n",
        "\n",
        "\n",
        "def my_collate(batch):\n",
        "    input_seqs = [seq_pair[0][1:-1] for seq_pair in batch]\n",
        "    output_input_seqs = [seq_pair[1][:-1] for seq_pair in batch]\n",
        "    output_output_seqs = [seq_pair[1][1:] for seq_pair in batch]\n",
        "    padded_input = nn.utils.rnn.pad_sequence(input_seqs, batch_first=True)\n",
        "    padded_output_input = nn.utils.rnn.pad_sequence(output_input_seqs, batch_first=True)\n",
        "    padded_output_output = nn.utils.rnn.pad_sequence(output_output_seqs, batch_first=True)\n",
        "    input_mask = get_mask(padded_input)\n",
        "    output_mask = get_mask(padded_output_input, decoding=True)\n",
        "\n",
        "    return padded_input, padded_output_input, padded_output_output, input_mask, output_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-mF7fFObZMo"
      },
      "source": [
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, en_data_path, ru_data_path, output='ru'):\n",
        "        super(TranslationDataset, self).__init__()\n",
        "        self.tokenizers = {'input': None, 'target': None}\n",
        "\n",
        "        en_type = 'output' if output=='en' else 'input'\n",
        "        ru_type = 'output' if output=='ru' else 'input'\n",
        "        en_data = self.read_and_preprocess(en_data_path, en_type)\n",
        "        ru_data = self.read_and_preprocess(ru_data_path, ru_type)\n",
        "\n",
        "        self.en_dataset, self.ru_dataset = self.sort(en_data, ru_data)\n",
        "\n",
        "    def read_and_preprocess(self, data_path, data_type):\n",
        "        with open(data_path, 'r') as f:\n",
        "            data = f.readlines()\n",
        "        data = [d[:d.find('\\n')] for d in data if not d.find('\\n')==-1]\n",
        "        dataset = []\n",
        "        \n",
        "\n",
        "        tokenizer = get_yttm_bpe(data_path, data_type)\n",
        "        self.tokenizers[data_type] = tokenizer\n",
        "        \n",
        "        for i, line in enumerate(data):\n",
        "            output = tokenizer.encode(line, output_type=yttm.OutputType.ID, bos=True, eos=True)\n",
        "            dataset.append(output)\n",
        "        \n",
        "        return dataset\n",
        "    \n",
        "    def sort(self, en_data, ru_data):\n",
        "        en_ru_sorted = sorted(zip(en_data, ru_data),\n",
        "                              key=lambda x:max(len(x[0]), len(x[1])))\n",
        "        return list(zip(*en_ru_sorted))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.en_dataset)\n",
        "    \n",
        "    @property\n",
        "    def vocab_size(self):\n",
        "        input_vocab_size = len(self.tokenizers['input'].vocab())\n",
        "        output_vocab_size = len(self.tokenizers['output'].vocab())\n",
        "        \n",
        "        return {'input': input_vocab_size,\n",
        "                'output': output_vocab_size}\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        return torch.tensor(self.en_dataset[idx]), torch.tensor(self.ru_dataset[idx])\n",
        "\n",
        "class RandomSortingSampler(Sampler):\n",
        "    def __init__(self, sorted_data, batch_size=32, shuffle=False):\n",
        "        super(RandomSortingSampler, self).__init__(sorted_data)\n",
        "        self.dataset_len = len(sorted_data)\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self._reset_sampler()\n",
        "        self.drop_last = shuffle\n",
        "\n",
        "    def _reset_sampler(self):\n",
        "        ids = range(self.dataset_len)\n",
        "        if self.shuffle:\n",
        "            ids = [ids[i:i + self.batch_size] for i in range(0, len(ids), self.batch_size) if i + self.batch_size < len(ids)]\n",
        "            random.shuffle(ids)\n",
        "            ids = list(chain.from_iterable(ids))\n",
        "        self.sampler = iter(ids)\n",
        "\n",
        "\n",
        "    def __iter__(self):\n",
        "        batch = []\n",
        "        for idx in self.sampler:\n",
        "            batch.append(idx)\n",
        "            if len(batch) == self.batch_size:\n",
        "                yield batch\n",
        "                batch = []\n",
        "        if len(batch) > 0 and not self.drop_last:\n",
        "            yield batch\n",
        "        self._reset_sampler()\n",
        "\n",
        "    def __len__(self):\n",
        "        if self.drop_last:\n",
        "            return self.dataset_len // self.batch_size  # type: ignore\n",
        "        else:\n",
        "            return (self.dataset_len + self.batch_size - 1) // self.batch_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSyg_MqJ8V5q"
      },
      "source": [
        "## Train:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKSxSEf5nIB5"
      },
      "source": [
        "### Training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XfHNwZSFliC"
      },
      "source": [
        "def lr_lbmd(cur_step, emb_size=512, warmup_steps=cfg_train_warmup_steps):\n",
        "    decay = emb_size**(-0.5) * min((cur_step + 1) **(-0.5), (cur_step + 1) * warmup_steps**(-1.5))\n",
        "    return decay\n",
        "\n",
        "def SCELoss(predicted, target, eps=0.01, pad=0):\n",
        "    '''\n",
        "    CE loss with smoothed labels\n",
        "    '''\n",
        "    predicted = predicted.permute(0,2,1)\n",
        "    K = predicted.shape[-1]\n",
        "    pad_mask = target == pad\n",
        "    with torch.no_grad():\n",
        "        ohe_target = torch.nn.functional.one_hot(target, K).float()\n",
        "        ohe_target *= (1 - eps)\n",
        "        ohe_target += eps / K\n",
        "        ohe_target[pad_mask] = 0\n",
        "        nrof_nonzero = (~pad_mask).sum()\n",
        "    return - (F.log_softmax(predicted, -1) * ohe_target).sum() / nrof_nonzero\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InHSc4hnwESa"
      },
      "source": [
        "class Train:\n",
        "    def __init__(self, traindataset=None, trainloader=None, \n",
        "                 valdataset=None, valloader=None, to_log=False):\n",
        "        self.trainloader = trainloader\n",
        "        self.train_dataset = traindataset\n",
        "        self.valloader = valloader\n",
        "        self.valdataset = valdataset\n",
        "\n",
        "        self.model = Transformer(traindataset.vocab_size['input'],\n",
        "                                 traindataset.vocab_size['output'])\n",
        "        self.model.to(cfg_train_device)\n",
        "        \n",
        "        # self.crit = nn.CrossEntropyLoss()\n",
        "        self.crit = SCELoss\n",
        "        self.optim = optim.Adam(self.model.parameters(), betas=(0.9, 0.98),\n",
        "                                eps=1e-9, lr=cfg_train_init_lr)\n",
        "        self.scheduler = LambdaLR(self.optim, lr_lambda=lr_lbmd)\n",
        "        self.nrof_epochs = cfg_train_nrof_epochs\n",
        "        self.epoch_size = len(self.train_dataset) // cfg_train_batch_size + 1\n",
        "        self.cur_epoch, self.global_step = 0, 0\n",
        "\n",
        "        self.to_log = to_log\n",
        "        if self.to_log:\n",
        "            self.train_writer = SummaryWriter(os.path.join(cfg_train_logs_dir, 'train'))\n",
        "            self.val_writer = SummaryWriter(os.path.join(cfg_train_logs_dir, 'val'))\n",
        "            self.best_loss = 1000.\n",
        "\n",
        "\n",
        "\n",
        "    def decode_sequences(self, sequences, seq_type='output'):\n",
        "        '''\n",
        "        sequences (numpy.ndarray): batch of numerical sequences\n",
        "        '''\n",
        "        tokenizer = self.train_dataset.tokenizers[seq_type]\n",
        "        sentences = tokenizer.decode(sequences)\n",
        "        return sentences\n",
        "\n",
        "\n",
        "    def save_model(self):\n",
        "        if not os.path.exists(os.path.dirname(cfg_train_ckpt_path)):\n",
        "            os.makedirs(os.path.dirname(cfg_train_ckpt_path))\n",
        "\n",
        "        torch.save({\"step\": self.global_step,\n",
        "                    \"model\": self.model.state_dict(),\n",
        "                    \"optimizer\": self.optim.state_dict(),\n",
        "                    \"scheduler\": self.scheduler.state_dict(),\n",
        "                    \"loss\": self.best_loss},\n",
        "                   cfg_train_ckpt_path)\n",
        "\n",
        "        print(\"Model saved...\")\n",
        "\n",
        "    def load_model(self):\n",
        "        ckpt = torch.load(cfg_train_ckpt_path)\n",
        "        self.cur_epoch = ckpt[\"step\"] // self.epoch_size + 1\n",
        "        self.global_step = ckpt[\"step\"] + 1\n",
        "        self.model.load_state_dict(ckpt[\"model\"])\n",
        "        self.optim.load_state_dict(ckpt[\"optimizer\"])\n",
        "        self.scheduler.load_state_dict(ckpt[\"scheduler\"])\n",
        "        self.best_loss = ckpt[\"loss\"]\n",
        "\n",
        "    def train_epoch(self):\n",
        "        t = time()\n",
        "        self.model.train()\n",
        "        nrof_samples, cur_loss = 0, 0.0\n",
        "\n",
        "        for batch_idx, batch in enumerate(self.trainloader):\n",
        "            input, target_input, target_output, input_mask, target_mask = batch\n",
        "            input = input.to(cfg_train_device)\n",
        "            target_input = target_input.to(cfg_train_device)\n",
        "            target_output = target_output.to(cfg_train_device)\n",
        "            input_mask = input_mask.to(cfg_train_device)\n",
        "            target_mask = target_mask.to(cfg_train_device)\n",
        "            self.optim.zero_grad()\n",
        "            outputs = self.model(input, target_input, input_mask, target_mask)\n",
        "\n",
        "            loss = self.crit(outputs.permute(0,2,1), target_output)\n",
        "            loss.backward()\n",
        "\n",
        "            cur_loss += loss.item()\n",
        "            nrof_samples += len(input)\n",
        "\n",
        "            self.optim.step()\n",
        "            self.scheduler.step()\n",
        "            self.global_step += 1\n",
        "\n",
        "            if batch_idx % cfg_train_log_interval == 0 and batch_idx!=0: \n",
        "                print('Batch num:', batch_idx)\n",
        "                decoded_inputs = self.decode_sequences(input[:4,:].detach().cpu().numpy().tolist(), seq_type='input')\n",
        "                decoded_outputs = self.decode_sequences(torch.argmax(outputs[:4,:,:], dim=-1).detach().cpu().numpy().tolist())\n",
        "                decoded_targets = self.decode_sequences(target_output[:4,:].detach().cpu().numpy().tolist())\n",
        "                print(\"Train loss: {:.4f}\".format(cur_loss/ nrof_samples))\n",
        "                print('decoded_inputs', decoded_inputs)\n",
        "                print('decoded_targets', decoded_targets)\n",
        "                print('decoded_outputs', decoded_outputs)\n",
        "\n",
        "                if self.to_log:\n",
        "                    self.train_writer.add_scalar('Loss', cur_loss / nrof_samples, self.global_step)\n",
        "                    self.train_writer.add_scalar('LR', self.optim.state_dict()[\"param_groups\"][0][\"lr\"], self.global_step)\n",
        "                \n",
        "                nrof_samples, cur_loss = 0, 0.0\n",
        "            \n",
        "    def validate(self):\n",
        "        self.model.eval()\n",
        "        nrof_samples, cur_loss = 0, 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, batch in enumerate(self.valloader):\n",
        "                input, target_input, target_output, input_mask, target_mask = batch\n",
        "                input = input.to(cfg_train_device)\n",
        "                target_input = target_input.to(cfg_train_device)\n",
        "                target_output = target_output.to(cfg_train_device)\n",
        "                input_mask = input_mask.to(cfg_train_device)\n",
        "                target_mask = target_mask.to(cfg_train_device)\n",
        "\n",
        "                outputs = self.model(input, target_input, input_mask, target_mask)\n",
        "\n",
        "                loss = self.crit(outputs.permute(0,2,1), target_output)\n",
        "                cur_loss += loss.item()\n",
        "                nrof_samples += len(input)\n",
        "\n",
        "                if batch_idx==0:\n",
        "                    decoded_inputs = self.decode_sequences(input[:4,:].detach().cpu().numpy().tolist(), seq_type='input')\n",
        "                    decoded_outputs = self.decode_sequences(torch.argmax(outputs[:4,:,:], dim=-1).detach().cpu().numpy().tolist())\n",
        "                    decoded_targets = self.decode_sequences(target_output[:4,:].detach().cpu().numpy().tolist())\n",
        "\n",
        "                    print('decoded_inputs', decoded_inputs)\n",
        "                    print('decoded_targets', decoded_targets)\n",
        "                    print('decoded_outputs', decoded_outputs)\n",
        "                    \n",
        "\n",
        "        return cur_loss/nrof_samples\n",
        "\n",
        "    def train(self):\n",
        "        if cfg_train_load:\n",
        "            self.load_model()\n",
        "\n",
        "        for epoch in range(self.cur_epoch, self.cur_epoch + self.nrof_epochs):\n",
        "            self.train_epoch()\n",
        "            val_loss = self.validate()\n",
        "            print(\"Epoch {} trained\\nVal loss: {:.4f}\".format(epoch, val_loss))\n",
        "            if self.to_log:\n",
        "                self.val_writer.add_scalar('Loss', val_loss, self.global_step)\n",
        "            self.save_model()\n",
        "\n",
        "    \n",
        "    def _get_eval_out(self, encoded_input, decoded_sequence):\n",
        "        stacked_decoded_sequence = torch.tensor(decoded_sequence).unsqueeze(0).to(cfg_train_device)\n",
        "        target_mask = get_mask(stacked_decoded_sequence, decoding=True)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = self.model.decode(encoded_input, stacked_decoded_sequence, target_mask)\n",
        "        return output\n",
        "\n",
        "\n",
        "    def evaluate_greedy(self, sequence, stop_predict_count=30, bos=1, eos=2):\n",
        "        '''\n",
        "        Вывод должен совпадать с результатом evaluate_beam() при beam=1 \n",
        "        '''\n",
        "        self.load_model()\n",
        "        self.model.eval()\n",
        "        current_symbol, decoded_sequence = bos, [bos]\n",
        "        input_mask = get_mask(sequence)\n",
        "        encoded_input = self.model.encode(sequence, input_mask)\n",
        "\n",
        "        while not current_symbol==eos and len(decoded_sequence) < stop_predict_count:\n",
        "            output = self._get_eval_out(encoded_input, decoded_sequence)\n",
        "            decoded_outputs = torch.argmax(output[:, :, :], dim=-1)\n",
        "            current_symbol = decoded_outputs[0][len(decoded_sequence) - 1].cpu().numpy().tolist()\n",
        "            decoded_sequence.append(current_symbol)\n",
        "\n",
        "\n",
        "        return decoded_sequence\n",
        "\n",
        "    def evaluate_beam(self, sequence, stop_predict_count=30, beam=3, bos=1, eos=2):\n",
        "        self.load_model()\n",
        "        self.model.eval()\n",
        "        input_mask = get_mask(sequence)\n",
        "        encoded_input = self.model.encode(sequence, input_mask)\n",
        "        current_best_dec_probs = [([bos], 0)]\n",
        "        current_leaves = []\n",
        "        current_beam = beam\n",
        "\n",
        "\n",
        "        while any([len(d[0]) < stop_predict_count and not eos in d[0] for d in current_best_dec_probs]) \\\n",
        "                and current_beam>0:\n",
        "            all_log_probs = []\n",
        "\n",
        "            for d in current_best_dec_probs:\n",
        "                decoding, log_prob = d\n",
        "                \n",
        "                if len(decoding) < stop_predict_count and not eos in decoding:\n",
        "                    output = self._get_eval_out(encoded_input, decoding)\n",
        "                    output = output.detach().cpu().numpy().tolist()\n",
        "                    all_log_probs.extend([(decoding + [i], x + log_prob) for i, x in enumerate(output[0][len(decoding)-1])])\n",
        "\n",
        "\n",
        "            all_log_probs = sorted(all_log_probs, key=lambda x: (x[1]) / len(x[0]), reverse=True)[:current_beam]\n",
        "            current_leaves.extend([all_log_probs.pop(i) for i, x in enumerate(all_log_probs) if x[0][-1] == eos])\n",
        "            current_beam = beam - len(current_leaves)\n",
        "            current_best_dec_probs = copy.deepcopy(all_log_probs)\n",
        "\n",
        "        decoded_sequence = max([x for x in current_best_dec_probs + current_leaves], key=lambda y: y[1]/len(y[0]))[0]\n",
        "        return decoded_sequence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ATLiuX57SEi"
      },
      "source": [
        "### Train and evaluate:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_k9e0sW7WvT"
      },
      "source": [
        "#### Digits:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uN3R-xKO7hZ0"
      },
      "source": [
        "# Data:\n",
        "train_digit_sequences, val_digit_sequences = get_digit_data()\n",
        "train_dataset = DigitDataset(train_digit_sequences, 10)\n",
        "val_dataset = DigitDataset(val_digit_sequences, 10)\n",
        "train_loader = DataLoader(train_dataset, batch_size=cfg_train_batch_size, collate_fn=my_collate)\n",
        "val_loader = DataLoader(val_dataset, batch_size=cfg_train_batch_size, collate_fn=my_collate)\n",
        "TR = Train(train_dataset, train_loader, val_dataset, val_loader, to_log=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TY3SNDPREMp_"
      },
      "source": [
        "# Check train:\n",
        "TR.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfXdggUI7Yn3"
      },
      "source": [
        "# Check inference:\n",
        "sequence = torch.randint(3,10,(1, 20)).to(cfg_train_device)\n",
        "print('Input sentence: ', [1] + sequence.cpu().numpy().tolist()[0] + [2])\n",
        "greedy_out = TR.evaluate_greedy(sequence)\n",
        "beam_out = TR.evaluate_beam(sequence, beam=4)\n",
        "print('Predicted greedy:', greedy_out)\n",
        "print('Predicted beam:', beam_out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3MZXdAI7r5U"
      },
      "source": [
        "#### Anki dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sHOLqXn7t0n"
      },
      "source": [
        "# Data:\n",
        "if not (os.path.exists('/content/en_train_set')\\\n",
        "        and os.path.exists('/content/ru_train_set')):\n",
        "    split_anki_dataset()\n",
        "    \n",
        "train_dataset = TranslationDataset('/content/en_train_set', '/content/ru_train_set')\n",
        "val_dataset = TranslationDataset('/content/en_val_set', '/content/ru_val_set')\n",
        "test_dataset = TranslationDataset('/content/en_test_set', '/content/ru_test_set')\n",
        "train_loader = DataLoader(train_dataset,\n",
        "                          batch_sampler=RandomSortingSampler(\n",
        "                              train_dataset, batch_size=cfg_train_batch_size,\n",
        "                              shuffle=True),\n",
        "                          collate_fn=my_collate)\n",
        "val_loader = DataLoader(val_dataset, batch_size=cfg_train_batch_size,\n",
        "                            collate_fn=my_collate)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1,\n",
        "                            collate_fn=my_collate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjemtnsALQI7"
      },
      "source": [
        "# Check train:\n",
        "TR = Train(train_dataset, train_loader, val_dataset, val_loader, to_log=True)\n",
        "TR.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctC_BSJwWJcp"
      },
      "source": [
        "# Check inference:\n",
        "#TR = Train(train_dataset, train_loader, val_dataset, val_loader, to_log=True)\n",
        "sentence = 'It doesn\\'t work.'\n",
        "tokenised_sentece = TR.train_dataset.tokenizers['input'].encode(sentence, output_type=yttm.OutputType.ID, bos=True,\n",
        "                                                                eos=True)\n",
        "print('Input tokens:', tokenised_sentece)\n",
        "greedy_out = TR.evaluate_greedy(torch.tensor(tokenised_sentece[1:-1]).unsqueeze(0).to(cfg_train_device),\n",
        "                                stop_predict_count=15, bos=2, eos=3)\n",
        "beam_out = TR.evaluate_beam(torch.tensor(tokenised_sentece[1:-1]).unsqueeze(0).to(cfg_train_device),\n",
        "                            stop_predict_count=15, beam=5, bos=2, eos=3)\n",
        "decoded_greedy_out = TR.train_dataset.tokenizers['output'].decode(greedy_out)\n",
        "decoded_beam_out = TR.train_dataset.tokenizers['output'].decode(beam_out)\n",
        "print('Predicted greedy: {}\\nPredicted beam: {}'.format(decoded_greedy_out[0].replace('<PAD>',''),\n",
        "                                                        decoded_beam_out[0].replace('<PAD>','')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6P0CbZ4umAzV"
      },
      "source": [
        "##### Count BLEU score:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgivzqxDmFvG"
      },
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mlJmgepmIrM"
      },
      "source": [
        "def count_bleu(test_dataset):\n",
        "    TR = Train(train_dataset, train_loader, val_dataset, val_loader)\n",
        "\n",
        "    ru_tokenizer = TR.train_dataset.tokenizers['output']\n",
        "    en_tokenizer = TR.train_dataset.tokenizers['input']\n",
        "\n",
        "    target_sentences, predicted_sentences = [], []\n",
        "    print('test len', len(test_dataset))\n",
        "    \n",
        "    t = time()\n",
        "    for i, d in enumerate(test_dataset):\n",
        "        en_ids, ru_ids = d \n",
        "        out = TR.evaluate_beam(en_ids[1:-1].unsqueeze(0).to(cfg_train_device), stop_predict_count=10, beam=3, bos=2, eos=3)\n",
        "        decoded_out = TR.train_dataset.tokenizers['output'].decode(out[1:-1])\n",
        "        \n",
        "        target_sentences.append(ru_tokenizer.decode(ru_ids[1:-1].numpy().tolist())[0])\n",
        "        predicted_sentences.append(decoded_out[0].replace('<PAD>',''))\n",
        "        if i%50==0:\n",
        "            print('En input: {}\\nRu target: {}\\nRu predicted: {}\\nTime spent:{}'.format(\n",
        "               en_tokenizer.decode(en_ids[1:-1].numpy().tolist())[0],\n",
        "               target_sentences[-1],\n",
        "               predicted_sentences[-1],\n",
        "               time() - t\n",
        "            ))\n",
        "            t = time()\n",
        "            bleu_score = corpus_bleu(target_sentences, predicted_sentences, weights=(0.5, 0.5, 0, 0))\n",
        "            print('Bleu score: {:.2f} on {} pairs'.format(bleu_score,\n",
        "                                                          len(target_sentences)))\n",
        "\n",
        "    bleu_score = corpus_bleu(target_sentences, predicted_sentences)\n",
        "    print('Bleu score: {:.2f}'.format(bleu_score))\n",
        "\n",
        "\n",
        "count_bleu(test_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iMPQlrxiB1K"
      },
      "source": [
        "### Logs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvkL3Ms_pxcj"
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "getsnqm7pyf-"
      },
      "source": [
        "%tensorboard --logdir '/content/drive/MyDrive/NLP/Transformer/anki/logs'"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}